{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align: center\">\n",
    "<img src=\"../../assets/images/dtlogo.png\" alt=\"Duckietown\" width=\"50%\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object Detection\n",
    "\n",
    "Machine-learned object detection models can be extremely useful. They are faster and often more reliable than traditional computer vision models. Additionally, we can use pretrained model weights to cut down immensely on training time.\n",
    "\n",
    "Here's an example of what an object detector might output:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<iframe width=\"800\" height=\"500\"\n",
    "src=\"https://www.youtube.com/embed/3jD02dxL6gg\" \n",
    "frameborder=\"0\" \n",
    "allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" \n",
    "allowfullscreen\n",
    "style=\"margin: auto; display: block\"></iframe>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In this exercise, you will create your own Duckietown object detection dataset. You will learn about the general structure such a dataset should follow. You will train the object detection model on that dataset ([in a subsequent notebook](../03-Training/training.ipynb). Finally, you will integrate the model into a ROS node and test the integration ([in the last notebook](../04-Integration/integration.ipynb), so that your Duckiebot knows how to recognize duckie pedestrians (and thus avoid them). You can test your object detector in simulation and on your real Duckiebot.\n",
    "\n",
    "### Steps:\n",
    "\n",
    "1. Setup  \n",
    "2. Investigation\n",
    "3. Data collection\n",
    "4. Training\n",
    "5. Integration\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "skip"
    ]
   },
   "source": [
    "## 1. Setup\n",
    "\n",
    "First, we need some global variables. These allow you to change the directory where we store the data you will need. You can also change the image size to reflect what your final model uses, but you can worry about that later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_DIR=\"/code/object-detection/assets/duckietown_object_detection_dataset\"\n",
    "IMAGE_SIZE = 416\n",
    "# this is the percentage of real data that will go into the training set (as opposed to the testing set)\n",
    "REAL_TRAIN_TEST_SPLIT_PERCENTAGE = 0.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "skip"
    ]
   },
   "source": [
    "While you will build your own dataset with simulated images in part 2, it would be unreasonable to ask you to build your own dataset of real images. Run the cell below to download a dataset of pre-labelled real images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm -rf /code/object-detection/assets/duckietown_object_detection_dataset/*\n",
      "mkdir -p /code/object-detection/assets/duckietown_object_detection_dataset/images\n",
      "mkdir -p /code/object-detection/assets/duckietown_object_detection_dataset/labels\n",
      "mkdir -p /code/object-detection/assets/duckietown_object_detection_dataset/train/images\n",
      "mkdir -p /code/object-detection/assets/duckietown_object_detection_dataset/train/labels\n",
      "mkdir -p /code/object-detection/assets/duckietown_object_detection_dataset/val/images\n",
      "mkdir -p /code/object-detection/assets/duckietown_object_detection_dataset/val/labels\n",
      "--2024-02-06 06:32:47--  https://duckietown-public-storage.s3.amazonaws.com/assets/mooc/2022/duckietown_object_detection_dataset.zip\n",
      "Resolving duckietown-public-storage.s3.amazonaws.com (duckietown-public-storage.s3.amazonaws.com)... 52.217.50.28, 54.231.224.241, 52.217.202.137, ...\n",
      "Connecting to duckietown-public-storage.s3.amazonaws.com (duckietown-public-storage.s3.amazonaws.com)|52.217.50.28|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 703630821 (671M) [binary/octet-stream]\n",
      "Saving to: ‘/tmp/dataset.zip’\n",
      "\n",
      "/tmp/dataset.zip      2%[                    ]  14.08M  60.5KB/s    eta 93m 5s ^C\n",
      "unzip -q /tmp/dataset.zip -d $(dirname /code/object-detection/assets/duckietown_object_detection_dataset)\n",
      "[/tmp/dataset.zip]\n",
      "  End-of-central-directory signature not found.  Either this file is not\n",
      "  a zipfile, or it constitutes one disk of a multi-part archive.  In the\n",
      "  latter case the central directory and zipfile comment will be found on\n",
      "  the last disk(s) of this archive.\n",
      "unzip:  cannot find zipfile directory in one of /tmp/dataset.zip or\n",
      "        /tmp/dataset.zip.zip, and cannot find /tmp/dataset.zip.ZIP, period.\n",
      "\n",
      "rm /tmp/dataset.zip\n"
     ]
    }
   ],
   "source": [
    "from utils.misc import runp\n",
    "\n",
    "# download dataset\n",
    "if not os.path.exists(DATASET_DIR):\n",
    "    runp(f\"rm -rf {DATASET_DIR}/*\")\n",
    "    runp(f\"mkdir -p {DATASET_DIR}/images\")\n",
    "    runp(f\"mkdir -p {DATASET_DIR}/labels\")\n",
    "    runp(f\"mkdir -p {DATASET_DIR}/train/images\")\n",
    "    runp(f\"mkdir -p {DATASET_DIR}/train/labels\")\n",
    "    runp(f\"mkdir -p {DATASET_DIR}/val/images\")\n",
    "    runp(f\"mkdir -p {DATASET_DIR}/val/labels\")\n",
    "else:\n",
    "    print(\"Folder structure already exists!\")\n",
    "\n",
    "# download dataset\n",
    "if not os.path.exists(f\"{DATASET_DIR}/frames\"):\n",
    "    !wget -O /tmp/dataset.zip https://duckietown-public-storage.s3.amazonaws.com/assets/mooc/2022/duckietown_object_detection_dataset.zip\n",
    "    runp(f\"unzip -q /tmp/dataset.zip -d $(dirname {DATASET_DIR})\")\n",
    "    runp(f\"rm /tmp/dataset.zip\")\n",
    "else:\n",
    "    print(\"Dataset already downloaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These real-world images are not the right size. Run the cell bellow to resize them (and resize the associated bounding boxes accordingly).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from utils.misc import xminyminxmaxymax2xywfnormalized, train_test_split, makedirs, runp\n",
    "\n",
    "with open(f\"{DATASET_DIR}/annotation/final_anns.json\") as anns:\n",
    "    annotations = json.load(anns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npz_index = 0\n",
    "\n",
    "all_image_names = []\n",
    "    \n",
    "def save_img(img, boxes, classes):\n",
    "    global npz_index\n",
    "    cv2.imwrite(f\"{DATASET_DIR}/images/real_{npz_index}.jpg\", img)\n",
    "    with open(f\"{DATASET_DIR}/labels/real_{npz_index}.txt\", \"w\") as f:\n",
    "        for i in range(len(boxes)):\n",
    "            f.write(f\"{classes[i]} \"+\" \".join(map(str,boxes[i]))+\"\\n\")\n",
    "    npz_index += 1\n",
    "    all_image_names.append(f\"real_{npz_index}\")\n",
    "\n",
    "filenames = tqdm(os.listdir(f\"{DATASET_DIR}/frames\"))\n",
    "for filename in filenames:\n",
    "    img = cv2.imread(f\"{DATASET_DIR}/frames/{filename}\")\n",
    "\n",
    "    orig_y, orig_x = img.shape[0], img.shape[1]\n",
    "    scale_y, scale_x = IMAGE_SIZE/orig_y, IMAGE_SIZE/orig_x\n",
    "\n",
    "    img = cv2.resize(img, (IMAGE_SIZE,IMAGE_SIZE))\n",
    "\n",
    "    boxes = []\n",
    "    classes = []\n",
    "\n",
    "    if filename not in annotations:\n",
    "        continue\n",
    "\n",
    "    for detection in annotations[filename]:\n",
    "        box = detection[\"bbox\"]\n",
    "        label = detection[\"cat_name\"]\n",
    "\n",
    "        if label not in [\"duckie\", \"cone\"]:\n",
    "            continue\n",
    "\n",
    "        orig_x_min, orig_y_min, orig_w, orig_h = box\n",
    "\n",
    "        x_min = int(np.round(orig_x_min * scale_x))\n",
    "        y_min = int(np.round(orig_y_min * scale_y))\n",
    "        x_max = x_min + int(np.round(orig_w * scale_x))\n",
    "        y_max = y_min + int(np.round(orig_h * scale_y))\n",
    "\n",
    "        boxes.append([x_min, y_min, x_max, y_max])\n",
    "        classes.append(1 if label == \"duckie\" else 2)\n",
    "\n",
    "    if len(boxes) == 0:\n",
    "        continue\n",
    "\n",
    "\n",
    "    boxes = np.array([xminyminxmaxymax2xywfnormalized(box, IMAGE_SIZE) for box in boxes])\n",
    "    classes = np.array(classes)-1\n",
    "    \n",
    "    save_img(img, boxes, classes)\n",
    "\n",
    "\n",
    "\n",
    "train_test_split(all_image_names, REAL_TRAIN_TEST_SPLIT_PERCENTAGE, DATASET_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once that's done, you're all set! We'll explain how the code above worked as you continue through this notebook.\n",
    "\n",
    "## 2. Investigation\n",
    "\n",
    "What does an object detection dataset look like? Clearly, the specifics will depend on the convention used by specific models, but the general idea is intuitive:\n",
    "\n",
    "- We need an image\n",
    "- This image might have many bounding boxes in it, so we need some sort of list of coordinates\n",
    "- These bounding boxes must be associated with a class\n",
    "\n",
    "How are the bounding boxes defined?\n",
    "\n",
    "![image of a bounding box](../../assets/images/bbox.png)\n",
    "\n",
    "Some conventions use `x_min y_min width height`, whereas others use `x_min y_min x_max y_max`, and others use `x_center y_center width height`. In this exercise, the model we recommend ([YoloV5](https://github.com/Velythyl/yolov5)) uses `x_center y_center width height`.\n",
    "\n",
    "And how do we actually obtain these bounding boxes? In real-life applications, you would need to label a dataset of images by hand. But if you have access to a simulator that is able to segment images, you could obtain the bounding boxes directly from the segmented images. \n",
    "\n",
    "If you take a look at Pytorch's object detection [tutorial](https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html), that is similar to what they do. While their images were segmented by hand, the tutorial uses the same technique that we will use here to obtain the bounding boxes. Their images look like this:\n",
    "\n",
    "![image with bounding boxes](../../assets/images/FudanPed.png)\n",
    "\n",
    "And they simply calculate the min and max x and y coordinates of the segmented objects to obtain the bounding box.\n",
    "\n",
    "We will use the segmented mode in the Duckietown simulator to compute the bounding boxes of non-segmented images.\n",
    "\n",
    "#### What we want to detect\n",
    "\n",
    "The goal of this exercise is to make Duckietown safer: we want to be able to detect duckie pedestrians on the road and avoid squishing them. We also want to detect trucks, buses, and cones. Here is the complete list, along with their corresponding IDs:\n",
    "\n",
    "| Object    | ID    |\n",
    "| ---       | ---   |\n",
    "| Duckie    | 0     |\n",
    "| Cone      | 1     |\n",
    "| Truck     | 2     |\n",
    "| Bus       | 3     |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data collection\n",
    "\n",
    "\n",
    "### Tool and Format\n",
    "\n",
    "We are going to supplement the data from the real dataset that we already downloaded with data automatically *generated from the simulator*.  The script we will use for this is the [data_collection.py](../../packages/utils/data_collection.py) file. You can edit the file in order to change the some parameters on what to generate, e.g. the number of images generated, which maps to be used by the simulator. More parameters (with description) can be found in the file, under the comment line:\n",
    "\n",
    "![img](../../assets/images/setup_instr_data_col_0.png)\n",
    "\n",
    "\n",
    "The purpose of the [data_collection.py](../../packages/utils/data_collection.py) script is to automatically generate data for you from the simulator. \n",
    "In the rest of this activity we will walk step by step through the process. \n",
    "\n",
    "Of course, your dataset's format depends heavily on your model. If you want to use the [YoloV5](https://github.com/duckietown/yolov5) model that we suggest, you should closely follow their [guide on how to train using custom data](https://github.com/ultralytics/yolov5/wiki/Train-Custom-Data). Your data should follow the following directory structure:\n",
    "\n",
    "![image of dataset save format](../../assets/images/dataset_format.png)\n",
    "\n",
    "The dataset is called `duckietown_object_detection_dataset` and is stored inside the `assets/` directory of this learning experience.\n",
    "We have created two subdirectories in that folder: `train` and `val`. Both these directories should contain two subdirectories, `images` and `labels`. Inside `images`, you must place your images, and inside `labels`, you must place the images' bounding boxes data. Notice that the label files use the same name as their corresponding image files but with a different extension. In other words, the data for `0.jpg` can be found in `0.txt`.\n",
    "\n",
    "The format for the label files is fairly simple. For each bounding box in the corresponding image, write a row of the form `class x_center y_center width height`. Keep in mind that the pixel data must be 0-to-1 normalized (i.e., you can calculate the usual `x_center y_center width height` in pixel space and divide by the image's size). For example,\n",
    "\n",
    "    0 0.5 0.5 0.2 0.2\n",
    "    1 0.60 0.70 0.4 0.2\n",
    "\n",
    "this says \"there is a duckie (class 0) centered in the image, whose width and height are 20% of the image's. There is also a cone (class 1) whose center is at 60% of the image's maximal x value and 70% of the image's maximal y value, and its width is 40% of the image's own while its height is 20%.\"\n",
    "\n",
    "Again, it is recommended that you read the guide posted on YoloV5's GitHub: [guide on how to train using custom data](https://github.com/ultralytics/yolov5/wiki/Train-Custom-Data)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running data collection\n",
    "\n",
    "After you're done editing the [data_collection.py](../../packages/utils/data_collection.py) file, we will need to run it against the simulator.\n",
    "We will do that through this activity's virtual desktop environment. (Run the following 2 commands from the root directory of this activity)\n",
    "\n",
    "* First, let's prepare the environment for this activity to run in.\n",
    "\n",
    "    ```shell\n",
    "    dts code build\n",
    "    ```\n",
    "\n",
    "* Then, access this activity's virtual desktop (also known and referred to as `VNC`) by ,\n",
    "\n",
    "    ```shell\n",
    "    dts code workbench --simulation --launcher data-collection\n",
    "    ```\n",
    "\n",
    "* Open the VNC with the URL that you see on screen, among the terminal logs\n",
    "    * Cmd/Ctrl + Click on the link, or\n",
    "    * Copy-paste the link to your browser address bar\n",
    "    * The link looks like this:\n",
    "    ![](../../assets/images/setup_instr_vnc.png)\n",
    "\n",
    "\n",
    "\n",
    "Then, click the \"Data Collection\" icon on the desktop. \n",
    "This will run your [data_collection.py](../../packages/utils/data_collection.py) script file.\n",
    "\n",
    "You will see a simulator view and the agent moving while collecting images. The process will load different maps and randomize time of day, starting location and heading and other aspects of the environment. Wait until the simulator view window and the terminal terminate and close automatically.\n",
    "\n",
    "* This is what you should expect to see. The collection progress is shown in the terminal. The image view shows the segmented scene, while we save the original image for training (see next section). \n",
    "    ![](../../assets/images/setup_instr_data_col_1.png)\n",
    "\n",
    "Later, if you edit the script again in this editor, you need to close the \"Data Collection\" application and click on the icon again for the changes to have an effect. Also, remove the images in `assets/duckietown_object_detection_dataset/[train|val]/images` folders, which do NOT start with `real_`.\n",
    "\n",
    "Once satisfied, you can close the VNC browser tab and terminate the `dts code workbench` command above by pressing Ctrl-C in your terminal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data generation/collection procedure explained\n",
    "\n",
    "In this section, we take some sample images and illustrate how the data collection script generates labels for the data from our simulator.\n",
    "\n",
    "#### 1. Take the segmented image (this is provided to you by the simulator's rendering engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "mapping = {\n",
    "    \"house\": \"3deb34\",\n",
    "    \"bus\": \"ebd334\",\n",
    "    \"truck\": \"961fad\",\n",
    "    \"duckie\": \"cfa923\",\n",
    "    \"cone\": \"ffa600\",\n",
    "    \"floor\": \"000000\",\n",
    "    \"grass\": \"000000\",\n",
    "    \"barrier\": \"000099\"\n",
    "}\n",
    "mapping = {\n",
    "    key:\n",
    "        [int(h[i:i+2], 16) for i in (0,2,4)]\n",
    "    for key, h in mapping.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Feel free to experiment with a few other files in the images folder. All of the original/segmented pairs are labeled as *_not_seg and *_seg\n",
    "obs = np.asarray(Image.open('../../assets/images/duckie_not_seg.png'))\n",
    "obs_seg = np.asarray(Image.open('../../assets/images/duckie_seg.png'))\n",
    "# define the mapping from objects to colours\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(obs_seg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Remove colors we are not interested in\n",
    "\n",
    "The function below removes all colors that do not match the given class name.\n",
    "We use this to isolate the objects of interest by isolating their respective color first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from solution.setup_activity import segmented_image_one_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test it out by removing everything that is not a duckie in the image above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duckie_masked_image = segmented_image_one_class(np.asarray(obs_seg),\"duckie\")\n",
    "plt.imshow(duckie_masked_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Find bounding boxes around each unique instance within the image\n",
    "\n",
    "The function below isolates the object by finding the contours of the colored blob in the image above.\n",
    "\n",
    "This results in the bounding box around the object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from solution.setup_activity import find_all_bboxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below takes the original image and computed bounding boxes and superimposes the bounding boxes to the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def show_image_with_boxes(img, boxes):\n",
    "    import matplotlib.patches as patches\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.imshow(img)\n",
    "    for box in boxes:\n",
    "        rect = patches.Rectangle((box[0], box[2]), box[1]-box[0], box[3]-box[2], linewidth=1, edgecolor='w', facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test these functions out on the image above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boxes = find_all_bboxes(duckie_masked_image)\n",
    "show_image_with_boxes(obs,boxes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Let's do that but for all classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from solution.setup_activity import find_all_boxes_and_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_boxes, all_classes = find_all_boxes_and_classes(obs_seg)\n",
    "show_image_with_boxes(obs, all_boxes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we save the ***non-segmented*** version of the image, and write its bounding boxes + their classes to a corresponding txt file. The above steps demonstrate what is implemented in the [data_collection.py](../../packages/utils/data_collection.py) file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining with the real dataset & training/test set splits\n",
    "\n",
    "When training supervised learning models, one must worry about overfitting to the training set. If you can keep *some* of your dataset *out* of your training data, you can use it to verify that your model does not overfit to your dataset by *testing* it on the data you left out. We call this chunk of data the *validation set*. \n",
    "\n",
    "You can experiment with the `REAL_TRAIN_TEST_SPLIT_PERCENTAGE` variable defined at the top of this notebook. Tune its value to adjust the percentage of the **real** data that is used for training as opposed to testing. There is a similar variable defined in [data_collection.py](../../packages/utils/data_collection.py), called `SIMULATED_TRAIN_SPLIT_PERCENTAGE` which controls the percentage of the **simulated** data that will be used for training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next step\n",
    "\n",
    "You can continue with the [Training notebook](../03-Training/training.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
