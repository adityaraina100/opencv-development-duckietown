{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align: center\">\n",
    "<img src=\"../../assets/images/dtlogo.png\" alt=\"Duckietown\" width=\"50%\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object detection for robots\n",
    "\n",
    "## Integration\n",
    "\n",
    "Finally, we need to integrate our model with ROS. We will do so by editing the file [`integration_activity.py`](../../packages/solution/integration_activity.py).\n",
    "\n",
    "\n",
    "### Simulation ðŸ’»\n",
    "If you don't have a Jetson Nano Duckiebot, you can run this exercise locally with the simulator. This will run your code as a Pytorch model. This means that we rely on your host machine's CPU or GPU to run your model.\n",
    "\n",
    "### Hardware ðŸš™\n",
    "\n",
    "If you have a Jetson Nano Duckiebot, you can run this exercise on your Duckiebot. This will convert your model to a TensorRT model, and run it your Jetson Nano's GPU.\n",
    "\n",
    "### What we are doing in this notebook\n",
    "\n",
    "In both cases, we need to edit the ROS node to decide how we will use the detections.\n",
    "Should you call your model on every image from your camera? Only once every 4-5 frames, to preserve your CPU?\n",
    "You might also want to change your robot's behaviour depending on the size of the detection.\n",
    "If it is small, the object is probably far away, so there's no need to stop.\n",
    "\n",
    "To get started, open the [`integration_activity.py`](../../packages/solution/integration_activity.py) file in the `packages/solution` directory.\n",
    "\n",
    "You'll first need to input your information into the `DT_TOKEN` and `MODEL_NAME` functions. Follow the `TODO` markers in both functions to copy in your Duckietown token and the name of your uploaded model from the previous notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now compile our agent with the neural network model we trained and the behavior we implemented in the [`integration_activity.py`](../../packages/solution/integration_activity.py) file above.\n",
    "\n",
    "You can build your agent by running the command,\n",
    "\n",
    "```\n",
    "dts code build\n",
    "```\n",
    "\n",
    "Then, you can use the following command to test your agent in simulation. The behavior of the driving agent here is to drive in a straight line until a detection triggers a stop. You will be able to control how detections can turn into stop signals by editing the [`integration_activity.py`](../../packages/solution/integration_activity.py) file. More on this later in the notebook.\n",
    "\n",
    "Let us run the simulated agent to see our neural network at work.\n",
    "\n",
    "```\n",
    "dts code workbench --sim\n",
    "```\n",
    "\n",
    "If you have a Duckiebot, run the following commands to test object detection in the real world:\n",
    "\n",
    "```\n",
    "dts code workbench -b <DUCKIEBOT_NAME>\n",
    "```\n",
    "\n",
    "In both cases, you will be given a VNC url in the terminal. Click on it to visualize in real-time the objects that the neural network we just trained detected in the camera image. In the VNC desktop that opens up, click on the \"Object Detection\" icon.\n",
    "\n",
    "<p style=\"text-align:center;\"><img src=\"../../assets/images/object_detection_vnc_icon.png\" width=\"160\" alt=\"object-detection-icon\"></p>  \n",
    "\n",
    "A stream of images from our agent's camera will appear, with the bounding boxes detected by our freshly trained neural network superimposed. An example is shown below.\n",
    "\n",
    "<p style=\"text-align:center;\"><img src=\"../../assets/images/rqt_with_duckie.png\" width=\"450\" alt=\"rqt-with-duckie\"></p>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "\n",
    "### Framerate\n",
    "\n",
    "While object detection is useful, it is also very expensive, computationally.\n",
    "\n",
    "One trick used to reduce said cost is to only call the full model infrequently.\n",
    "For example, one might call the model only a few times a second, which is very slow in\n",
    "computer timeframes, but relatively fast for the real world. \n",
    "\n",
    "Of course, this varies from application to application. In very dynamic, fast\n",
    "robotic environments, clearly the model should be called more frequently.\n",
    "\n",
    "You can fine-tune this yourself: edit the `NUMBER_FRAMES_SKIPPED` function in the [`integration_activity.py`](../../packages/solution/integration_activity.py) file to indicate the number of frames\n",
    "your robot should skip before calling its object detection model. A (default) value of `0` indicates that no frames are skipped (i.e., all frames are fed to the neural network for object detection), a value of `1` means that we are detecting objects every other frame, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "In real life, we would use a full neural network model to produce very accurate\n",
    "predictions, and then a less accurate model coupled with a Kalman filter (or\n",
    "other such estimation system) to \"track\" each prediction during the skipped frames.\n",
    "\n",
    "For this exercises, we will limit ourselves to just skipping frames."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering\n",
    "\n",
    "Some of your predictions might not actually be useful. For example, in Duckietown,\n",
    "the trucks and busses are always parked on the side of the road. Your robot will\n",
    "never have to avoid or stop for them.\n",
    "\n",
    "Cones can be in the road in some maps, but for this exercises, you can assume that there\n",
    "aren't any.\n",
    "\n",
    "#### Filtering by class\n",
    "\n",
    "For this reason, you probably want to remove all non-duckies from your predictions,\n",
    "since only duckies will be on the road. Update the `filter_by_classes` function to do this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Filtering by score\n",
    "\n",
    "Depending on the model, you might also want to remove very unconfident detections.\n",
    "\n",
    "Then again, your model might not be confident even for detections that are absolutely\n",
    "correct. You should experiment with the `filter_by_scores` function to find a value that works well for your model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Filtering by bounding box\n",
    "\n",
    "Finally, you should also evaluate what each detection *means* in terms of positionning.\n",
    "\n",
    "If a bounding box is in the leftmost or rightmost thirds of the image, it might be the case that\n",
    "the object in not even on the road, and that your robot would be able to go by it without issue.\n",
    "\n",
    "<p style=\"text-align:center;\"><img src=\"../../assets/images/thirds.png\" width=\"450\" alt=\"successful-test-odometry\"></p>  \n",
    "\n",
    "Also, if a bounding box's area is small, its object is likely far away. So there is no need to\n",
    "try to avoid the object or stop the robot: the robot still has a bit of driving to do before it reaches\n",
    "the object. So filtering out small detections might be a good idea too.  Update the `filter_by_bboxes` function to play around with this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "### Fine-tuning\n",
    "\n",
    "In all of the functions above, there is not objective right answer. You should play with\n",
    "your functions and fine-tune their behaviours. Robotics is an iterative process!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission\n",
    "\n",
    "You can test your agent locally with\n",
    "\n",
    "    $ dts code evaluate\n",
    "    \n",
    "\n",
    "And then finally submit with \n",
    "\n",
    "    $ dts code submit\n",
    "    \n",
    "\n",
    "And then check out how you did [on the leaderboard](https://challenges.duckietown.org/v4/humans/challenges/lx22-objdet/leaderboard). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
